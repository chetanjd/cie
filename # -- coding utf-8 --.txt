# -*- coding: utf-8 -*-
"""
Created on Tue Aug  6 20:51:10 2024

@author: CHETANJD
"""

import os
import cv2
import numpy as np
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.preprocessing import image
from keras.models import Model

def load_images_from_folder(folder):
    images = []
    filenames = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            images.append(img)
            filenames.append(filename)
    return images, filenames

# Specify the path to your image dataset
folder_path = 'C:/Users/HP/Desktop/skindisease_datasets/sample'
images, filenames = load_images_from_folder(folder_path)

# Load pre-trained VGG16 model + higher level layers
base_model = VGG16(weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)

def extract_features(img):
    img = cv2.resize(img, (224, 224))  # Resize image to 224x224 as required by VGG16
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)
    features = model.predict(img)
    return features.flatten()

features_list = []
for img in images:
    features_list.append(extract_features(img))

features_array = np.array(features_list)

from sklearn.metrics.pairwise import cosine_similarity

def find_similar_images(query_image_index, features_array, top_n=5):
    similarities = cosine_similarity([features_array[query_image_index]], features_array)[0]
    indices = np.argsort(similarities)[::-1][1:top_n + 1]  # Skip the first one as it is the query image itself
    return indices, similarities[indices]

# Example: Find top 5 images similar to the first image in the dataset
query_image_index = 0
similar_indices, similar_scores = find_similar_images(query_image_index, features_array)

print(f"Query Image: {filenames[query_image_index]}")
for idx, score in zip(similar_indices, similar_scores):
    print(f"Similar Image: {filenames[idx]} with similarity score: {score:.4f}")

import matplotlib.pyplot as plt

def plot_images(query_image_index, similar_indices):
    plt.figure(figsize=(10, 5))

    plt.subplot(1, 6, 1)
    plt.imshow(cv2.cvtColor(images[query_image_index], cv2.COLOR_BGR2RGB))
    plt.title('Query Image')
    plt.axis('off')

    for i, idx in enumerate(similar_indices, start=2):
        plt.subplot(1, 6, i)
        plt.imshow(cv2.cvtColor(images[idx], cv2.COLOR_BGR2RGB))
        plt.title(f'Similar {i-1}')
        plt.axis('off')

    plt.show()
plot_images(query_image_index, similar_indices)



# -*- coding: utf-8 -*-
"""
Created on Tue Aug  6 20:42:48 2024

@author: CHETANJD
"""
import tensorflow
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical

# Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Normalize the images
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout

def alexnet_model(input_shape, num_classes):
    model = Sequential()

    # 1st Convolutional Layer
    model.add(Conv2D(filters=96, input_shape=input_shape, kernel_size=(11,11), strides=(4,4), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

    # 2nd Convolutional Layer
    model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

    # 3rd Convolutional Layer
    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))

    # 4th Convolutional Layer
    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))

    # 5th Convolutional Layer
    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

    # Flatten the layers
    model.add(Flatten())

    # 1st Fully Connected Layer
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))

    # 2nd Fully Connected Layer
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))

    # Output Layer
    model.add(Dense(num_classes, activation='softmax'))

    return model

input_shape = (32, 32, 3)  # CIFAR-10 image size
num_classes = 10  # CIFAR-10 has 10 classes

model = alexnet_model(input_shape, num_classes)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_test, y_test), verbose=1)

score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])



# -*- coding: utf-8 -*-
"""
Created on Tue Aug  6 20:37:50 2024

@author: CHETANJD
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = load_iris()
X = data.data 
y = data.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test)

#Implementing AdaBoost with Different Base Learners

#AdaBoost with Decision Tree
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Decision Tree as base learner
base_learner = DecisionTreeClassifier(max_depth=1)
adaboost_dt =AdaBoostClassifier(base_estimator=base_learner, n_estimators=50,random_state=42)

# Fit the model
adaboost_dt.fit(X_train, y_train)

# Predict
y_pred_dt = adaboost_dt.predict(X_test)

# Evaluate
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f'AdaBoost with Decision Tree accuracy: {accuracy_dt}')

#AdaBoost with Logistic Regression
from sklearn.linear_model import LogisticRegression

# Logistic Regression as base learner
base_learner_lr = LogisticRegression(max_iter=1000)
adaboost_lr = AdaBoostClassifier(base_estimator=base_learner_lr, n_estimators=50,random_state=42)

# Fit the model
adaboost_lr.fit(X_train, y_train)

# Predict
y_pred_lr = adaboost_lr.predict(X_test)

# Evaluate
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f'AdaBoost with Logistic Regression accuracy: {accuracy_lr}')

#Out Put :-AdaBoost with Logistic Regression accuracy: 0.9

#Gradient Boosting with Decision Tree

from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting with Decision Tree
gbc = GradientBoostingClassifier(n_estimators=50, random_state=42)

# Fit the model
gbc.fit(X_train, y_train)

# Predict
y_pred_gbc = gbc.predict(X_test)

# Evaluate
accuracy_gbc = accuracy_score(y_test, y_pred_gbc) 
print(f'Gradient Boosting accuracy:{accuracy_gbc}')

#Out Put:-Gradient Boosting accuracy: 1.0

#Implementing XGBoost

import xgboost as xgb

# XGBoost
xgb_model = xgb.XGBClassifier(n_estimators=50, random_state=42)

# Fit the model
xgb_model.fit(X_train, y_train)

# Predict
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate
accuracy_xgb = accuracy_score(y_test, y_pred_xgb) 
print(f'XGBoost accuracy:{accuracy_xgb}')



from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score,classification_report

df = load_iris()
X = df.data
y = df.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(),n_estimators=50,random_state=42)

bag_clf.fit(X_train,y_train)

y_pred=bag_clf.predict(X_test)
y_pred

acc = accuracy_score(y_test,y_pred)
print(f"Accuracy:{acc*100:.2f}%")


df = load_iris()
X = df.data
y = df.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

base_learners = {"DecisionTree":DecisionTreeClassifier(),"KNN":KNeighborsClassifier(),"SVM":SVC(probability=True)}

for name,model in base_learners.items():
    bag_model=BaggingClassifier(estimator=model,n_estimators=10,random_state=42)
    bag_model.fit(X_train,y_train)
    y_pred=bag_model.predict(X_test)
    acc = accuracy_score(y_test,y_pred)
    print(f"Accuracy of {name}:{acc*100:.2f}%")
    print(f"Classification_Report of {name}:---")
    print(classification_report(y_test, y_pred))
    
    
"""import os
import cv2
import numpy as np

import matplotlib.pyplot as plt

def load_images_from_folder(folder):
    images = []
    filenames = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            images.append(img)
            filenames.append(filename)
    return images, filenames

def extract_features(images):
    hog = cv2.HOGDescriptor()
    features = []
    for image in images:
        # Resize image to a fixed size
        resized_img = cv2.resize(image, (128, 128))
        # Convert to grayscale
        gray_img = cv2.cvtColor(resized_img, cv2.COLOR_RGB2GRAY)
        # Compute HOG features
        hog_features = hog.compute(gray_img)
        features.append(hog_features.flatten())
    return np.array(features)

folder='D:\\6th_semester\Advanced_AIML\ADV Lab\sample'
img,label=load_images_from_folder(folder)
features = extract_features(img)
labels = np.array(label)

features

labels

X_train,X_test,y_train,y_test = train_test_split(features,labels,test_size=0.4,random_state=42)

bag_clf.fit(X_train,y_train)

y_pred=bag_clf.predict(X_test)
y_pred

acc = accuracy_score(y_test,y_pred)
print(f"Accuracy:{acc*100:.2f}%")

# Evaluate the classifier
print(classification_report(y_test, y_pred))"""


# -*- coding: utf-8 -*-
"""
Created on Tue Aug  6 20:33:34 2024

@author: CHETANJD
"""
import pandas as pd
from sklearn.cluster import Birch
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create and fit the BIRCH model
birch_model = Birch(n_clusters=3)
birch_model.fit(X_scaled)
labels = birch_model.predict(X_scaled)

# Create a DataFrame with the scaled data and the cluster labels
df = pd.DataFrame(X_scaled, columns=iris.feature_names)
df['Cluster'] = labels

# Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', hue='Cluster', palette='viridis')
plt.title('BIRCH Clustering on Iris Dataset')
plt.show()

# Evaluate the clustering
conf_matrix = confusion_matrix(y, labels)
print('Confusion Matrix:\n', conf_matrix)

acc_score = accuracy_score(y, labels)
print('Accuracy Score:', acc_score)


import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# Generate sample data
centers = [[1, 1], [-1, -1], [1, -1]]
X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)

# Compute DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(X)

core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print(f'Estimated number of clusters: {n_clusters_}')
print(f'Estimated number of noise points: {n_noise_}')

# Plot result
# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

plt.title(f'Estimated number of clusters: {n_clusters_}')
plt.show()



# -*- coding: utf-8 -*-
"""
Created on Fri May 31 16:42:52 2024

@author: CHETANJD
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,roc_curve,auc
from sklearn.decomposition import PCA

lfw = fetch_lfw_people(min_faces_per_person=70,resize=0.4)

X = lfw.data
y = lfw.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#Apply PCA
n_components = 150
pca = PCA(n_components=n_components,whiten=True,random_state=42)
X_train_PCA = pca.fit_transform(X_train)
X_test_PCA = pca.fit_transform(X_test)

svm_class = SVC(kernel='rbf', C=1000.0,gamma=0.001,probability=True,random_state=42)

svm_class.fit(X_train_PCA,y_train)

#Predict the probabilities for each class.. 
y_score = svm_class.predict_proba(X_test_PCA)

y_pred = svm_class.predict(X_test_PCA)

accuracy = accuracy_score(y_test,y_pred)
print("Accuracy:--",accuracy)

#ROC Curve and ROC Area computation
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(len(lfw.target_names)):
    fpr[i],tpr[i],_=roc_curve(np.array(y_test == i), y_score[:,i])
    roc_auc[i] = auc(fpr[i],tpr[i])
    
#plot roc 
plt.figure(figsize=(8,6))
for i in range(len(lfw.target_names)):
    plt.plot(fpr[i],tpr[i],label='ROC Curve for class {} (AUC = {:.2f})'.format(lfw.target_names[i],roc_auc[i]))
    

plt.plot([0,1],[0,1],'k--',lw=2)
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()


from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC #Support Vector Classifier...
from sklearn.metrics import accuracy_score

df = load_iris()

X = df.data
y = df.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=42)

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

svm_class = SVC(kernel='linear')

svm_class.fit(X_train,y_train)

y_pred = svm_class.predict(X_test)

accuracy = accuracy_score(y_test,y_pred)
print("Accuracy:",accuracy)